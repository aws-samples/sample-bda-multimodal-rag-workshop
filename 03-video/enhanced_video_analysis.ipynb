{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Content Understanding: Advanced Video Analysis with Amazon Bedrock\n",
    "\n",
    "Welcome to the video analysis module of our multimodal data processing journey with Amazon Bedrock Data Automation (BDA). In previous modules, we explored foundation building with document analysis and seeing beyond text with image analysis, and unlocking the voice of information with audio analysis. Now, we're diving into perhaps the most complex and information-rich modality: video.\n",
    "\n",
    "## Why Video Analysis Matters\n",
    "\n",
    "Video represents the most information-dense form of content, combining visual elements, motion, audio, and text into a temporal flow. This richness makes video exceptionally valuable but also challenging to process with traditional methods.\n",
    "\n",
    "Consider that:\n",
    "- A single minute of video contains approximately 1,800 frames (at 30 fps)\n",
    "- The average enterprise has thousands of hours of video content that remains largely unsearchable\n",
    "- Manual video analysis costs $15-25 per minute of processed content\n",
    "- Only 1-2% of video content is typically leveraged in business intelligence systems\n",
    "\n",
    "Amazon Bedrock Data Automation transforms this landscape by enabling us to automatically:\n",
    "- Detect and analyze distinct scenes and shots\n",
    "- Generate comprehensive video summaries and chapter breakdowns\n",
    "- Extract text visible within the video frames\n",
    "- Identify logos and brands\n",
    "- Apply content moderation across visual and audio components\n",
    "- Classify content using standardized IAB categories\n",
    "\n",
    "This capability fundamentally changes how we interact with video content, unlocking insights that were previously trapped in the visual medium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Our Environment\n",
    "\n",
    "Let's begin by installing required libraries and importing dependencies. We'll be using our enhanced utility functions that incorporate the \"Dynamic Content Understanding\" theme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install \"boto3>=1.37.4\" \"matplotlib\" \"moviepy\" --upgrade -qq\n",
    "\n",
    "# Import necessary libraries\n",
    "import boto3\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from IPython.display import Video, clear_output, HTML, display, Markdown\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our video utilities from the consolidated utils module\n",
    "from utils.utils import BDAVideoUtils, show_business_context, ensure_bda_results_dir\n",
    "\n",
    "# Initialize our utility class\n",
    "bda_utils = BDAVideoUtils()\n",
    "print(f\"Setup complete. BDA utilities initialized for region: {bda_utils.current_region}\")\n",
    "print(f\"Using S3 bucket: {bda_utils.default_bucket}\")\n",
    "\n",
    "# Display business context for video analysis\n",
    "show_business_context(\"video_complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Sample Video\n",
    "\n",
    "First, we'll download a sample video and upload it to S3 for processing with BDA. We'll use a short video that contains various elements that BDA can analyze, including different scenes, spoken content, and visual elements.\n",
    "\n",
    "The video will be stored in an S3 bucket that BDA can access. This step follows the same pattern we used for document, image, and audio processing, where we first need to have the content accessible in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample video using our enhanced utility function\n",
    "sample_video = 'content-moderation-demo.mp4'\n",
    "source_url = 'https://ws-assets-prod-iad-r-pdx-f3b3f9f1a7d6a3d0.s3.us-west-2.amazonaws.com/335119c4-e170-43ad-b55c-76fa6bc33719/NetflixMeridian.mp4'\n",
    "\n",
    "# Download the video with enhanced error handling\n",
    "try:\n",
    "    bda_utils.download_video(source_url, sample_video)\n",
    "    print(f\"Successfully downloaded video to {sample_video}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading video: {e}\")\n",
    "\n",
    "# Display the video in the notebook for preview\n",
    "display(Video(sample_video, width=800))\n",
    "\n",
    "# Upload to S3 for BDA processing\n",
    "s3_key = f'{bda_utils.data_prefix}/{sample_video}'\n",
    "s3_uri = bda_utils.upload_to_s3(sample_video, s3_key)\n",
    "print(f\"Uploaded video to S3: {s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define BDA Configuration and Create Project\n",
    "\n",
    "Now we'll define the standard output configuration for video analysis and create a BDA project. This configuration determines what information BDA will extract from the video.\n",
    "\n",
    "### Video Processing Capabilities\n",
    "\n",
    "BDA offers specialized processing options for video content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display business context for video chapters\n",
    "show_business_context(\"chapter_detection\")\n",
    "\n",
    "# Define standard output configuration for video processing\n",
    "standard_output_config = {\n",
    "    'video': {\n",
    "        'extraction': {\n",
    "            'category': {\n",
    "                'state': 'ENABLED',\n",
    "                'types': [\n",
    "                    'CONTENT_MODERATION',  # Detect inappropriate content\n",
    "                    'TEXT_DETECTION',      # Extract text from the video\n",
    "                    'TRANSCRIPT',          # Generate transcript of spoken content\n",
    "                    'LOGOS'                # Identify brand logos\n",
    "                ]\n",
    "            },\n",
    "            'boundingBox': {\n",
    "                'state': 'ENABLED'         # Include bounding boxes for detected elements\n",
    "            }\n",
    "        },\n",
    "        'generativeField': {\n",
    "            'state': 'ENABLED',\n",
    "            'types': [\n",
    "                'VIDEO_SUMMARY',           # Generate overall video summary\n",
    "                'CHAPTER_SUMMARY',         # Generate summaries for each chapter\n",
    "                'IAB'                      # Classify into IAB categories\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a BDA project with our standard output configuration\n",
    "print(\"Creating BDA project for video analysis...\")\n",
    "response = bda_utils.bda_client.create_data_automation_project(\n",
    "    projectName=f'bda-workshop-video-project-{str(uuid.uuid4())[0:4]}',\n",
    "    projectDescription='BDA workshop video sample project',\n",
    "    projectStage='DEVELOPMENT',\n",
    "    standardOutputConfiguration=standard_output_config\n",
    ")\n",
    "\n",
    "# Get the project ARN\n",
    "video_project_arn = response.get(\"projectArn\")\n",
    "print(f\"BDA project created with ARN: {video_project_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process Video with BDA\n",
    "\n",
    "Now we'll use the `invoke_data_automation_async` API to process our video with BDA. As we've seen with document, image, and audio processing, BDA operates asynchronously due to the complexity and processing time required for rich media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke BDA to process the video\n",
    "print(f\"Processing video: {s3_uri}\")\n",
    "print(f\"Results will be stored at: s3://{bda_utils.default_bucket}/{bda_utils.output_prefix}\")\n",
    "\n",
    "# Call the invoke_data_automation_async API\n",
    "response = bda_utils.bda_runtime_client.invoke_data_automation_async(\n",
    "    inputConfiguration={\n",
    "        's3Uri': s3_uri  # The S3 location of our video\n",
    "    },\n",
    "    outputConfiguration={\n",
    "        's3Uri': f's3://{bda_utils.default_bucket}/{bda_utils.output_prefix}'  # Where to store results\n",
    "    },\n",
    "    dataAutomationConfiguration={\n",
    "        'dataAutomationProjectArn': video_project_arn,  # The project we created\n",
    "        'stage': 'DEVELOPMENT'                          # Must match the project stage\n",
    "    },\n",
    "    dataAutomationProfileArn=f'arn:aws:bedrock:{bda_utils.current_region}:{bda_utils.account_id}:data-automation-profile/us.data-automation-v1'\n",
    ")\n",
    "\n",
    "# Get the invocation ARN\n",
    "invocation_arn = response.get(\"invocationArn\")\n",
    "print(f\"Invocation ARN: {invocation_arn}\")\n",
    "\n",
    "# Wait for processing to complete using our enhanced pattern\n",
    "# This uses the same flexible pattern we developed for audio processing\n",
    "status_response = bda_utils.wait_for_completion(\n",
    "    get_status_function=bda_utils.bda_runtime_client.get_data_automation_status,\n",
    "    status_kwargs={'invocationArn': invocation_arn},\n",
    "    completion_states=['Success'],\n",
    "    error_states=['ClientError', 'ServiceError'],\n",
    "    status_path_in_response='status',\n",
    "    max_iterations=20,  # Video might take longer than other modalities\n",
    "    delay=10\n",
    ")\n",
    "\n",
    "# Check if processing was successful\n",
    "if status_response['status'] == 'Success':\n",
    "    output_config_uri = status_response.get(\"outputConfiguration\", {}).get(\"s3Uri\")\n",
    "    print(f\"\\nVideo processing completed successfully!\")\n",
    "    print(f\"Output configuration: {output_config_uri}\")\n",
    "else:\n",
    "    print(f\"\\nVideo processing failed with status: {status_response['status']}\")\n",
    "    if 'error_message' in status_response:\n",
    "        print(f\"Error message: {status_response['error_message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Retrieve and Explore BDA Results\n",
    "\n",
    "Now that the video has been processed, let's retrieve the results from S3 and explore the insights extracted by BDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load job metadata\n",
    "config_data = bda_utils.read_json_from_s3(output_config_uri)\n",
    "\n",
    "# Get standard output path\n",
    "standard_output_path = config_data[\"output_metadata\"][0][\"segment_metadata\"][0][\"standard_output_path\"]\n",
    "result_data = bda_utils.read_json_from_s3(standard_output_path)\n",
    "\n",
    "# Create bda-results directory if it doesn't exist\n",
    "ensure_bda_results_dir()\n",
    "\n",
    "# Save the result data to the bda-results directory\n",
    "with open('../bda-results/video_result.json', 'w') as f:\n",
    "    json.dump(result_data, f)\n",
    "    \n",
    "print(f\"Saved video results to: ../bda-results/video_result.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Video Metadata and Summary\n",
    "\n",
    "Let's first look at the basic metadata and the overall video summary generated by BDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display video metadata and summary\n",
    "print(\"=== Video Metadata ===\\n\")\n",
    "metadata = result_data[\"metadata\"]\n",
    "print(f\"Duration: {metadata.get('duration_seconds', 'N/A')} seconds\")\n",
    "print(f\"Format: {metadata.get('format', 'N/A')}\")\n",
    "print(f\"Resolution: {metadata.get('width_pixels', 'N/A')} x {metadata.get('height_pixels', 'N/A')}\")\n",
    "print(f\"Frame Rate: {metadata.get('frame_rate', 'N/A')} fps\")\n",
    "\n",
    "print(\"\\n=== Video Summary ===\\n\")\n",
    "if \"summary\" in result_data[\"video\"]:\n",
    "    print(result_data[\"video\"][\"summary\"])\n",
    "else:\n",
    "    print(\"No summary available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Chapter Information\n",
    "\n",
    "One of the most powerful capabilities of BDA for video analysis is automatic chapter detection and summarization. Let's retrieve the chapter structure of our video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed chapter information\n",
    "print(\"=== Chapter Information ===\\n\")\n",
    "for i, chapter in enumerate(result_data[\"chapters\"]):\n",
    "    start_time = chapter.get(\"start_timecode_smpte\", \"N/A\")\n",
    "    end_time = chapter.get(\"end_timecode_smpte\", \"N/A\")\n",
    "    print(f\"\\nChapter {i+1}: [{start_time} - {end_time}]\")\n",
    "    \n",
    "    if \"summary\" in chapter:\n",
    "        print(f\"Summary: {chapter['summary']}\")\n",
    "    \n",
    "    if \"iab_categories\" in chapter:\n",
    "        categories = [iab[\"category\"] for iab in chapter[\"iab_categories\"]]\n",
    "        print(f\"IAB Categories: {', '.join(categories)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Shot Transitions\n",
    "\n",
    "BDA also breaks down videos into individual shots, which are continuous segments from a single camera perspective. Let's analyze the shots detected in our video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about scene detection\n",
    "show_business_context(\"scene_detection\")\n",
    "\n",
    "# Display video shots with enhanced visualization\n",
    "print(\"=== Video Shot Analysis ===\\n\")\n",
    "print(\"Generating images for each shot in the video...\")\n",
    "shot_images = bda_utils.generate_shot_images(sample_video, result_data)\n",
    "bda_utils.plot_shots(shot_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Moderation Analysis\n",
    "\n",
    "BDA can detect potentially sensitive or inappropriate content in videos. Let's examine the content moderation results for our video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show business context for content moderation\n",
    "show_business_context(\"content_moderation\")\n",
    "\n",
    "# Display content moderation results with enhanced visualization\n",
    "print(\"=== Content Moderation Analysis ===\\n\")\n",
    "print(\"Displaying visual content moderation results for the first chapter:\")\n",
    "bda_utils.plot_content_moderation(sample_video, result_data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Detection in Video\n",
    "\n",
    "BDA can detect and extract text that appears in video frames. This is useful for capturing information displayed on screen, such as titles, captions, or other textual content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show business context for text detection\n",
    "show_business_context(\"video_text_detection\")\n",
    "\n",
    "# Function to extract text lines from a frame\n",
    "def extract_text_lines(frame):\n",
    "    text_lines = []\n",
    "    \n",
    "    # Check all possible locations where text might be stored\n",
    "    if \"features\" in frame and \"text_lines\" in frame[\"features\"]:\n",
    "        text_lines = frame[\"features\"][\"text_lines\"]\n",
    "    elif \"text_detection\" in frame:\n",
    "        text_lines = frame[\"text_detection\"]\n",
    "    elif \"text_lines\" in frame:\n",
    "        text_lines = frame[\"text_lines\"]\n",
    "    \n",
    "    return text_lines\n",
    "\n",
    "# Display detected text lines\n",
    "print(\"=== Detected Text in Video Frames ===\\n\")\n",
    "text_lines_found = False\n",
    "\n",
    "# Check for text in the frames\n",
    "for i, chapter in enumerate(result_data[\"chapters\"]):\n",
    "    for frame in chapter.get(\"frames\", []):\n",
    "        text_lines = extract_text_lines(frame)\n",
    "        \n",
    "        if text_lines:\n",
    "            text_lines_found = True\n",
    "            frame_time = frame[\"timestamp_millis\"] / 1000\n",
    "            print(f\"\\nText detected at {frame_time:.2f}s:\")\n",
    "            \n",
    "            for text_line in text_lines:\n",
    "                confidence = text_line.get(\"confidence\", \"N/A\")\n",
    "                detected_text = text_line.get(\"text\", \"\")\n",
    "                print(f\"- \\\"{detected_text}\\\" (Confidence: {confidence})\")\n",
    "\n",
    "if not text_lines_found:\n",
    "    print(\"No text detected in the video frames.\")\n",
    "else:\n",
    "    print(\"\\nüî§ Technical Win: BDA automatically extracted text from video frames!\")\n",
    "    print(\"This makes previously unsearchable text content in videos discoverable and analyzable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logo Detection\n",
    "\n",
    "BDA can identify logos and brand marks that appear in videos. This is valuable for brand monitoring, competitive analysis, and content monetization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show business context for logo detection\n",
    "show_business_context(\"logo_detection\")\n",
    "\n",
    "# Function to extract logos from a frame\n",
    "def extract_logos(frame):\n",
    "    logos = []\n",
    "    \n",
    "    # Check all possible locations where logos might be stored\n",
    "    if \"features\" in frame and \"logos\" in frame[\"features\"]:\n",
    "        logos = frame[\"features\"][\"logos\"]\n",
    "    elif \"logos\" in frame:\n",
    "        if isinstance(frame[\"logos\"], list):\n",
    "            logos = frame[\"logos\"]\n",
    "    \n",
    "    return logos\n",
    "\n",
    "# Display detected logos\n",
    "print(\"=== Detected Logos ===\\n\")\n",
    "logos_found = False\n",
    "\n",
    "# Check for logos in the frames\n",
    "for i, chapter in enumerate(result_data[\"chapters\"]):\n",
    "    for frame in chapter.get(\"frames\", []):\n",
    "        logos = extract_logos(frame)\n",
    "        \n",
    "        if logos:\n",
    "            logos_found = True\n",
    "            frame_time = frame[\"timestamp_millis\"] / 1000\n",
    "            print(f\"\\nLogos detected at {frame_time:.2f}s:\")\n",
    "            \n",
    "            for logo in logos:\n",
    "                confidence = logo.get(\"confidence\", \"N/A\")\n",
    "                logo_name = logo.get(\"name\", \"Unknown logo\")\n",
    "                print(f\"- \\\"{logo_name}\\\" (Confidence: {confidence})\")\n",
    "\n",
    "if not logos_found:\n",
    "    print(\"No logos detected in the video.\")\n",
    "else:\n",
    "    print(\"\\nüè¢ Technical Win: BDA automatically identified brand logos in the video!\")\n",
    "    print(\"This enables brand monitoring, competitive analysis, and content monetization opportunities.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IAB Category Analysis\n",
    "\n",
    "BDA can classify video content according to the Internet Advertising Bureau (IAB) content taxonomy. This provides standardized categorization for content discovery, ad targeting, and organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show business context for IAB categorization\n",
    "show_business_context(\"iab_categorization\")\n",
    "\n",
    "# Visualize IAB categories\n",
    "bda_utils.visualize_iab_categories(result_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting the Dots: From Images to Audio to Video\n",
    "\n",
    "Throughout this workshop series, we've explored how Amazon Bedrock Data Automation can extract structured insights from different content modalities:\n",
    "\n",
    "1. **Document Analysis**: We began with document extraction, learning how to transform static PDFs into structured data.\n",
    "\n",
    "2. **Image Analysis**: We moved beyond text to extract visual insights from images, detecting objects, text, and concepts.\n",
    "\n",
    "3. **Audio Analysis**: We unlocked the voice of information by processing spoken content and identifying speakers.\n",
    "\n",
    "4. **Video Analysis**: Now, we've seen how BDA can process the most complex modality - video - which combines visual, audio, and temporal elements into a rich information stream.\n",
    "\n",
    "The power of BDA comes from its ability to handle these diverse modalities through a consistent API pattern:\n",
    "\n",
    "```python\n",
    "# Create a project with appropriate configuration\n",
    "project_response = bda_client.create_data_automation_project(...)\n",
    "\n",
    "# Process content asynchronously\n",
    "invocation_response = bda_runtime_client.invoke_data_automation_async(...)\n",
    "\n",
    "# Wait for completion using our flexible pattern\n",
    "status_response = wait_for_completion(...)\n",
    "\n",
    "# Analyze results\n",
    "result_data = read_json_from_s3(output_path)\n",
    "```\n",
    "\n",
    "This consistent approach allows you to build applications that can extract insights from any content type, opening up new possibilities for content understanding, search, and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking Forward: From Understanding to Intelligence\n",
    "\n",
    "In the next module, we'll take the final step in our journey by combining all these modalities into a unified multimodal RAG (Retrieval-Augmented Generation) system. We'll see how the structured data extracted by BDA from documents, images, audio, and video can be integrated into a knowledge base for intelligent query answering.\n",
    "\n",
    "You'll learn how to:\n",
    "- Create a multimodal knowledge base that incorporates insights from all content types\n",
    "- Build intelligent query capabilities that can reference cross-modal content\n",
    "- Design applications that deliver comprehensive answers by synthesizing information from various sources\n",
    "- Create truly intelligent systems that understand not just individual modalities, but their relationships and contexts\n",
    "\n",
    "This final step will complete our journey from raw, unstructured data to actionable intelligence across the full spectrum of content types."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
