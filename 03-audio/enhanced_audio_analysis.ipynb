{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Analysis with Amazon Bedrock Data Automation: The Voice of Information\n",
    "\n",
    "This notebook continues your journey with Amazon Bedrock Data Automation (BDA) by exploring the rich world of audio data. While document processing provided the foundation for structured extraction, and image analysis allowed us to see beyond text, audio analysis represents another fundamental dimension of human communication: the voice.\n",
    "\n",
    "As you work through this notebook, you'll build capabilities to:\n",
    "- Convert spoken content into structured transcripts with speaker identification\n",
    "- Generate comprehensive summaries of audio content\n",
    "- Identify potentially sensitive or inappropriate content through moderation\n",
    "- Classify audio according to standard categories\n",
    "- Extract semantic insights from conversations and discussions\n",
    "\n",
    "This notebook demonstrates how to use Amazon Bedrock Data Automation (BDA) to analyze audio files and extract valuable insights. BDA can process audio to generate transcripts, identify speakers, detect content moderation issues, create summaries, and more.\n",
    "\n",
    "In this enhanced notebook, we'll focus on the core BDA workflow for audio analysis:\n",
    "\n",
    "1. Preparing a sample audio file\n",
    "2. Creating a BDA project with appropriate output configurations\n",
    "3. Processing the audio with BDA\n",
    "4. Analyzing the results (summaries, transcripts, and content moderation)\n",
    "\n",
    "The setup cell below contains helper functions and initialization code. **It's collapsed by default** - you can expand it to see the details, but you don't need to understand all of it to follow the main BDA workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This cell contains setup code and helper functions.\n",
    "# You can expand it to see the details, but you don't need to understand all of it.\n",
    "\n",
    "%pip install \"boto3>=1.37.4\" --upgrade -qq\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import os\n",
    "import sagemaker\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from IPython.display import Audio, clear_output, JSON, HTML, Markdown, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import all utilities from the consolidated utils module\n",
    "from utils.utils import BDAAudioUtils, show_business_context, ensure_bda_results_dir\n",
    "\n",
    "# Initialize our utility class\n",
    "bda_utils = BDAAudioUtils()\n",
    "\n",
    "# Display comprehensive business context for audio analysis\n",
    "show_business_context(\"audio_complete\")\n",
    "\n",
    "print(f\"Setup complete. BDA utilities initialized for region: {bda_utils.current_region}\")\n",
    "print(f\"Using S3 bucket: {bda_utils.bucket_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Industry Applications\n",
    "\n",
    "As you work through this notebook, consider how audio analysis could transform workflows in your specific domain:\n",
    "\n",
    "**Call Centers**: How would automatic transcription, speaker identification, and sentiment analysis of customer calls improve your service quality and agent performance?\n",
    "\n",
    "**Media & Entertainment**: What if your content libraries could automatically generate transcripts, summaries, and topic analysis for podcasts, interviews, and broadcasts?\n",
    "\n",
    "**Healthcare**: How could transcription of patient interactions, medical dictations, and automated documentation transform your clinical workflows?\n",
    "\n",
    "**Financial Services**: What insights could you gain from analyzing earnings calls, advisory conversations, and compliance monitoring of recorded interactions?\n",
    "\n",
    "**Your Industry**: What types of audio content are critical to your organization's workflows? What insights within these recordings would provide the most business value if automatically extracted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Sample Audio\n",
    "\n",
    "First, we'll download a sample audio file and upload it to S3 for processing with BDA. We'll use a podcast audio file that contains spoken content that BDA can analyze.\n",
    "\n",
    "The audio will be stored in an S3 bucket that BDA can access. This is a required step as BDA needs to read the audio from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample audio file\n",
    "sample_audio = 'podcastdemo.mp3'\n",
    "source_url = 'https://ws-assets-prod-iad-r-pdx-f3b3f9f1a7d6a3d0.s3.us-west-2.amazonaws.com/335119c4-e170-43ad-b55c-76fa6bc33719/podcastdemo.mp3'\n",
    "\n",
    "# Download the audio using our enhanced download function\n",
    "local_path = bda_utils.download_audio(source_url, sample_audio)\n",
    "\n",
    "# Display the audio player\n",
    "display(Audio(sample_audio, autoplay=False))\n",
    "\n",
    "# Upload to S3\n",
    "s3_key = f'{bda_utils.data_prefix}/{sample_audio}'\n",
    "s3_uri = bda_utils.upload_to_s3(sample_audio, s3_key)\n",
    "print(f\"Uploaded audio to S3: {s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define BDA Configuration and Create Project\n",
    "\n",
    "Now we'll define the standard output configuration for audio analysis and create a BDA project. This configuration determines what information BDA will extract from the audio.\n",
    "\n",
    "### Project Architecture for Audio Analysis\n",
    "\n",
    "### Key Configuration Options for Audio Analysis:\n",
    "\n",
    "- **Audio Content Moderation**: Detects inappropriate or unsafe content in the audio\n",
    "- **Topic Content Moderation**: Analyzes topics for potentially sensitive content\n",
    "- **Transcript**: Generates a transcript of spoken content with speaker identification\n",
    "- **Audio Summary**: Generates an overall summary of the audio content\n",
    "- **Topic Summary**: Provides summaries for each detected topic\n",
    "- **IAB Categories**: Classifies content into Internet Advertising Bureau categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show business context for project architecture\n",
    "show_business_context(\"project_architecture\")\n",
    "\n",
    "# Define standard output configuration for audio processing\n",
    "standard_output_config = {\n",
    "    \"audio\": {\n",
    "        \"extraction\": {\n",
    "            \"category\": {\n",
    "                \"state\": \"ENABLED\", \n",
    "                \"types\": [\n",
    "                    \"AUDIO_CONTENT_MODERATION\",  # Detect inappropriate content in audio\n",
    "                    \"TOPIC_CONTENT_MODERATION\",  # Analyze topics for sensitive content\n",
    "                    \"TRANSCRIPT\"                 # Generate transcript with speaker identification\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"generativeField\": {\n",
    "            \"state\": \"ENABLED\",\n",
    "            \"types\": [\n",
    "                \"AUDIO_SUMMARY\",                # Generate overall audio summary\n",
    "                \"TOPIC_SUMMARY\",                # Generate summaries for each topic\n",
    "                \"IAB\"                           # Classify into IAB categories\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a unique project name\n",
    "project_name = f'bda-workshop-audio-project-{str(uuid.uuid4())[0:4]}'\n",
    "\n",
    "# Create a BDA project with our standard output configuration\n",
    "print(\"Creating BDA project for audio analysis...\")\n",
    "response = bda_utils.bda_client.create_data_automation_project(\n",
    "    projectName=project_name,\n",
    "    projectDescription='BDA workshop audio sample project',\n",
    "    projectStage='DEVELOPMENT',\n",
    "    standardOutputConfiguration=standard_output_config\n",
    ")\n",
    "\n",
    "# Get the project ARN\n",
    "audio_project_arn = response.get(\"projectArn\")\n",
    "print(f\"BDA project created with ARN: {audio_project_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process Audio with BDA\n",
    "\n",
    "Now we'll use the `invoke_data_automation_async` API to process our audio with BDA. This API starts an asynchronous job to analyze the audio and extract insights based on our project configuration.\n",
    "\n",
    "The API returns an invocation ARN that we can use to check the status of the processing job. Audio processing can take several minutes depending on the length and complexity of the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show business context for processing pipeline\n",
    "show_business_context(\"processing_pipeline\")\n",
    "\n",
    "# Invoke BDA to process the audio\n",
    "print(f\"Processing audio: {s3_uri}\")\n",
    "print(f\"Results will be stored at: s3://{bda_utils.bucket_name}/{bda_utils.output_prefix}\")\n",
    "\n",
    "# Call the invoke_data_automation_async API\n",
    "response = bda_utils.bda_runtime_client.invoke_data_automation_async(\n",
    "    inputConfiguration={\n",
    "        's3Uri': s3_uri  # The S3 location of our audio\n",
    "    },\n",
    "    outputConfiguration={\n",
    "        's3Uri': f's3://{bda_utils.bucket_name}/{bda_utils.output_prefix}'  # Where to store results\n",
    "    },\n",
    "    dataAutomationConfiguration={\n",
    "        'dataAutomationProjectArn': audio_project_arn,  # The project we created\n",
    "        'stage': 'DEVELOPMENT'                          # Must match the project stage\n",
    "    },\n",
    "    dataAutomationProfileArn=f'arn:aws:bedrock:{bda_utils.current_region}:{bda_utils.account_id}:data-automation-profile/us.data-automation-v1'\n",
    ")\n",
    "\n",
    "# Get the invocation ARN\n",
    "invocation_arn = response.get(\"invocationArn\")\n",
    "print(f\"Invocation ARN: {invocation_arn}\")\n",
    "\n",
    "# Wait for processing to complete\n",
    "status_response = bda_utils.wait_for_completion(\n",
    "    get_status_function=bda_utils.bda_runtime_client.get_data_automation_status,\n",
    "    status_kwargs={'invocationArn': invocation_arn},\n",
    "    completion_states=['Success'],\n",
    "    error_states=['ClientError', 'ServiceError'],\n",
    "    status_path_in_response='status',\n",
    "    max_iterations=15,\n",
    "    delay=10\n",
    ")\n",
    "\n",
    "# Check if processing was successful\n",
    "if status_response['status'] == 'Success':\n",
    "    output_config_uri = status_response.get(\"outputConfiguration\", {}).get(\"s3Uri\")\n",
    "    print(f\"\\nAudio processing completed successfully!\")\n",
    "    print(f\"Output configuration: {output_config_uri}\")\n",
    "else:\n",
    "    print(f\"\\nAudio processing failed with status: {status}\")\n",
    "    if 'error_message' in status_response:\n",
    "        print(f\"Error message: {status_response['error_message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Access and Parse BDA Results\n",
    "\n",
    "Now we'll access the BDA results from S3 and parse them. The results include the audio summary, transcript, content moderation analysis, and other metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show business context for speaker identification\n",
    "show_business_context(\"speaker_identification\")\n",
    "\n",
    "# Extract the output location from the configuration URI\n",
    "bucket, key = bda_utils.get_bucket_and_key(output_config_uri)\n",
    "output_folder = os.path.dirname(key)\n",
    "result_key = f\"{output_folder}/0/standard_output/0/result.json\"\n",
    "\n",
    "# Create bda-results directory if it doesn't exist\n",
    "ensure_bda_results_dir()\n",
    "\n",
    "# Download the result file to the bda-results directory\n",
    "local_result_file = '../bda-results/audio_result.json'\n",
    "bda_utils.s3_client.download_file(bda_utils.bucket_name, result_key, local_result_file)\n",
    "print(f\"Downloaded result file to: {local_result_file}\")\n",
    "\n",
    "\n",
    "# Load the result data\n",
    "with open(local_result_file, 'r') as f:\n",
    "    result_data = json.load(f)\n",
    "\n",
    "# Display a preview of the result structure\n",
    "print(\"\\nBDA Result Structure:\")\n",
    "top_level_keys = list(result_data.keys())\n",
    "print(f\"Top-level keys: {', '.join(top_level_keys)}\")\n",
    "\n",
    "audio_keys = list(result_data.get('audio', {}).keys())\n",
    "print(f\"Audio section keys: {', '.join(audio_keys)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Audio Summary and Transcript\n",
    "\n",
    "Let's examine the audio summary and transcript generated by BDA. The summary provides a concise overview of the audio content, while the transcript captures the spoken content with speaker identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show business context for audio summarization\n",
    "show_business_context(\"audio_summarization\")\n",
    "\n",
    "# Display audio summary\n",
    "print(\"=== Audio Summary ===\\n\")\n",
    "if \"summary\" in result_data[\"audio\"]:\n",
    "    print(result_data[\"audio\"][\"summary\"])\n",
    "else:\n",
    "    print(\"No summary available\")\n",
    "\n",
    "# Display audio transcript\n",
    "print(\"\\n=== Audio Transcript ===\\n\")\n",
    "if \"transcript\" in result_data[\"audio\"] and \"representation\" in result_data[\"audio\"][\"transcript\"]:\n",
    "    transcript_data = result_data[\"audio\"][\"transcript\"]\n",
    "    print(transcript_data[\"representation\"][\"text\"])\n",
    "    \n",
    "    # Visualize speaker segments using our enhanced function\n",
    "    print(\"\\n=== Speaker Visualization ===\\n\")\n",
    "    bda_utils.visualize_transcript(transcript_data)\n",
    "else:\n",
    "    print(\"No transcript available\")\n",
    "\n",
    "# Display audio statistics\n",
    "if \"statistics\" in result_data:\n",
    "    print(\"\\n=== Audio Statistics ===\\n\")\n",
    "    print(json.dumps(result_data[\"statistics\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Content Moderation Results\n",
    "\n",
    "BDA provides content moderation analysis for audio, identifying potentially inappropriate or unsafe content. Let's examine these results to understand the content safety profile of our audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show business context for audio content moderation\n",
    "show_business_context(\"audio_content_moderation\")\n",
    "\n",
    "# Analyze content moderation results\n",
    "print(\"=== Content Moderation Analysis ===\\n\")\n",
    "bda_utils.analyze_content_moderation(result_data)\n",
    "\n",
    "# Generate visual summary of moderation scores\n",
    "print(\"\\n=== Content Moderation Score Visualization ===\\n\")\n",
    "bda_utils.generate_moderation_summary(result_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we demonstrated how to use Amazon Bedrock Data Automation (BDA) to analyze audio files and extract valuable insights. We covered the key steps in the BDA workflow:\n",
    "\n",
    "1. **Audio Preparation**: Uploaded a sample audio file to S3 for processing\n",
    "2. **Configuration**: Defined standard output configuration for audio analysis\n",
    "3. **Project Creation**: Created a BDA project with our configuration\n",
    "4. **Audio Processing**: Used the `invoke_data_automation_async` API to process the audio\n",
    "5. **Results Analysis**: Retrieved and analyzed the extracted insights\n",
    "\n",
    "### Key BDA APIs Used:\n",
    "- `create_data_automation_project`: Creates a project with configuration settings\n",
    "- `invoke_data_automation_async`: Processes an audio file asynchronously\n",
    "- `get_data_automation_status`: Checks the status of a processing job\n",
    "\n",
    "### BDA Capabilities Demonstrated:\n",
    "- Generating audio summaries\n",
    "- Creating transcripts with speaker identification\n",
    "- Detecting inappropriate content through content moderation\n",
    "- Analyzing audio statistics\n",
    "- Visualizing speaker distributions in conversations\n",
    "\n",
    "These capabilities can be used for various applications such as podcast analysis, call center analytics, meeting transcription, content moderation, and building audio intelligence solutions.\n",
    "\n",
    "## Looking Ahead: Dynamic Content Understanding\n",
    "\n",
    "Having mastered text, image, and audio processing, you're now ready to explore the most complex modality: video. In the next module, \"Dynamic Content Understanding\", you'll discover how BDA can process video content to extract insights from moving images, audio tracks, and scene transitions - combining multiple modalities into a comprehensive understanding of video content."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
